{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560140eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>model</th>\n",
       "      <th>material</th>\n",
       "      <th>author</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B005ZSSN10</td>\n",
       "      <td>DE</td>\n",
       "      <td>RED DRAGON Amberjack 3 - Steel Tip 22 Gramm Wo...</td>\n",
       "      <td>30.95</td>\n",
       "      <td>RED DRAGON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RDD0089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amberjacks Steel Dartpfeile sind verf√ºgbar in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B08PRYN6LD</td>\n",
       "      <td>DE</td>\n",
       "      <td>Simply Keto Lower Carb* Schokodrops ohne Zucke...</td>\n",
       "      <td>17.90</td>\n",
       "      <td>Simply Keto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750 g (1er Pack)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üå± NAT√úRLICHE S√úSSE DURCH ERYTHRIT - Wir stelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B09MBZJ48V</td>\n",
       "      <td>DE</td>\n",
       "      <td>Sennheiser 508377 PC 5.2 Chat, Stilvolles Mult...</td>\n",
       "      <td>68.89</td>\n",
       "      <td>Sennheiser</td>\n",
       "      <td>Multi-Colour</td>\n",
       "      <td>One size</td>\n",
       "      <td>508377</td>\n",
       "      <td>Kunstleder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5 MM BUCHSE - Kann problemlos an Ger√§te mit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B08ZN6F26S</td>\n",
       "      <td>DE</td>\n",
       "      <td>AmyBenton Auto ab 1 2 3 ahre - Baby Aufziehbar...</td>\n",
       "      <td>18.99</td>\n",
       "      <td>Amy &amp; Benton</td>\n",
       "      <td>Animal Car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008B</td>\n",
       "      <td>aufziehauto 1 jahr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>„ÄêAuto aufziehbar„Äë: Dr√ºcken Sie einfach leicht ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B094DGRV7D</td>\n",
       "      <td>DE</td>\n",
       "      <td>PLAYMOBIL - 70522 - Cavaliere mit grauem Pony</td>\n",
       "      <td>7.17</td>\n",
       "      <td>PLAYMOBIL</td>\n",
       "      <td>Nicht Zutreffend.</td>\n",
       "      <td>OneSize</td>\n",
       "      <td>70522</td>\n",
       "      <td>Polypropylen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inhalt: 1 St√ºck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id locale                                              title  \\\n",
       "0  B005ZSSN10     DE  RED DRAGON Amberjack 3 - Steel Tip 22 Gramm Wo...   \n",
       "1  B08PRYN6LD     DE  Simply Keto Lower Carb* Schokodrops ohne Zucke...   \n",
       "2  B09MBZJ48V     DE  Sennheiser 508377 PC 5.2 Chat, Stilvolles Mult...   \n",
       "3  B08ZN6F26S     DE  AmyBenton Auto ab 1 2 3 ahre - Baby Aufziehbar...   \n",
       "4  B094DGRV7D     DE      PLAYMOBIL - 70522 - Cavaliere mit grauem Pony   \n",
       "\n",
       "   price         brand              color              size    model  \\\n",
       "0  30.95    RED DRAGON                NaN               NaN  RDD0089   \n",
       "1  17.90   Simply Keto                NaN  750 g (1er Pack)      NaN   \n",
       "2  68.89    Sennheiser       Multi-Colour          One size   508377   \n",
       "3  18.99  Amy & Benton         Animal Car               NaN    2008B   \n",
       "4   7.17     PLAYMOBIL  Nicht Zutreffend.           OneSize    70522   \n",
       "\n",
       "             material author  \\\n",
       "0                 NaN    NaN   \n",
       "1                 NaN    NaN   \n",
       "2          Kunstleder    NaN   \n",
       "3  aufziehauto 1 jahr    NaN   \n",
       "4        Polypropylen    NaN   \n",
       "\n",
       "                                                desc  \n",
       "0  Amberjacks Steel Dartpfeile sind verf√ºgbar in ...  \n",
       "1  üå± NAT√úRLICHE S√úSSE DURCH ERYTHRIT - Wir stelle...  \n",
       "2  3.5 MM BUCHSE - Kann problemlos an Ger√§te mit ...  \n",
       "3  „ÄêAuto aufziehbar„Äë: Dr√ºcken Sie einfach leicht ...  \n",
       "4                                    Inhalt: 1 St√ºck  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"products_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82aee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1551057 entries, 0 to 1551056\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count    Dtype  \n",
      "---  ------    --------------    -----  \n",
      " 0   id        1551057 non-null  object \n",
      " 1   locale    1551057 non-null  object \n",
      " 2   title     1551049 non-null  object \n",
      " 3   price     1551057 non-null  float64\n",
      " 4   brand     1531691 non-null  object \n",
      " 5   color     1125432 non-null  object \n",
      " 6   size      917091 non-null   object \n",
      " 7   model     761137 non-null   object \n",
      " 8   material  834382 non-null   object \n",
      " 9   author    73509 non-null    object \n",
      " 10  desc      1424083 non-null  object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 130.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f262c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              price\n",
      "count  1.551057e+06\n",
      "mean   1.806107e+06\n",
      "std    8.302930e+06\n",
      "min    0.000000e+00\n",
      "25%    1.119000e+01\n",
      "50%    2.328000e+01\n",
      "75%    7.990000e+02\n",
      "max    4.000000e+07\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbba9efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>model</th>\n",
       "      <th>material</th>\n",
       "      <th>author</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>913336</th>\n",
       "      <td>B087LZNPHS</td>\n",
       "      <td>UK</td>\n",
       "      <td>SOCHOW Sherpa Fleece Throw Blanket, Double-Sid...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>SOCHOW</td>\n",
       "      <td>Teal Green</td>\n",
       "      <td>127cm√ó150cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% Polyester</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLOR: The sherpa throw blanket is available i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913337</th>\n",
       "      <td>B08THFN1KX</td>\n",
       "      <td>UK</td>\n",
       "      <td>Hippowarehouse Personalised Photo Printed Mous...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Hippowarehouse</td>\n",
       "      <td>White</td>\n",
       "      <td>240mm x 190mm x 60mm</td>\n",
       "      <td>50245-Mat-Perso</td>\n",
       "      <td>Rubber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Competitively priced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913338</th>\n",
       "      <td>0804185328</td>\n",
       "      <td>UK</td>\n",
       "      <td>500 Easy Recipes for Every Machine, Both Stove...</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Clarkson Potter</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scarbrough, Mark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913339</th>\n",
       "      <td>B09VBKDBW6</td>\n",
       "      <td>UK</td>\n",
       "      <td>TYHJOY Mini Bag Sealer, Handheld Vacuum Heat S...</td>\n",
       "      <td>11.99</td>\n",
       "      <td>TYHJOY</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FBA-sealer-black</td>\n",
       "      <td>Acrylonitrile Butadiene Styrene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>„ÄêAFTER-SALE„ÄëThis handheld food heat sealer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913340</th>\n",
       "      <td>B096ZW8B49</td>\n",
       "      <td>UK</td>\n",
       "      <td>Lucosobie Steering Wheel Lock - Car Anti-Theft...</td>\n",
       "      <td>26.99</td>\n",
       "      <td>Lucosobie</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alloy Steel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üîê„Äê Anti-Friction &amp; Customer First„ÄëEach box of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id locale                                              title  \\\n",
       "913336  B087LZNPHS     UK  SOCHOW Sherpa Fleece Throw Blanket, Double-Sid...   \n",
       "913337  B08THFN1KX     UK  Hippowarehouse Personalised Photo Printed Mous...   \n",
       "913338  0804185328     UK  500 Easy Recipes for Every Machine, Both Stove...   \n",
       "913339  B09VBKDBW6     UK  TYHJOY Mini Bag Sealer, Handheld Vacuum Heat S...   \n",
       "913340  B096ZW8B49     UK  Lucosobie Steering Wheel Lock - Car Anti-Theft...   \n",
       "\n",
       "        price            brand       color                  size  \\\n",
       "913336  24.99           SOCHOW  Teal Green           127cm√ó150cm   \n",
       "913337   9.95   Hippowarehouse       White  240mm x 190mm x 60mm   \n",
       "913338  16.49  Clarkson Potter       White                   NaN   \n",
       "913339  11.99           TYHJOY       Black                   NaN   \n",
       "913340  26.99        Lucosobie       Black                   NaN   \n",
       "\n",
       "                   model                         material            author  \\\n",
       "913336               NaN                   100% Polyester               NaN   \n",
       "913337   50245-Mat-Perso                           Rubber               NaN   \n",
       "913338               NaN                              NaN  Scarbrough, Mark   \n",
       "913339  FBA-sealer-black  Acrylonitrile Butadiene Styrene               NaN   \n",
       "913340               NaN                      Alloy Steel               NaN   \n",
       "\n",
       "                                                     desc  \n",
       "913336  COLOR: The sherpa throw blanket is available i...  \n",
       "913337                               Competitively priced  \n",
       "913338                                                NaN  \n",
       "913339  „ÄêAFTER-SALE„ÄëThis handheld food heat sealer sho...  \n",
       "913340  üîê„Äê Anti-Friction & Customer First„ÄëEach box of ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_samples = df[df['locale'] == 'UK'].copy()\n",
    "\n",
    "# Print the extracted samples\n",
    "uk_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0ee0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>model</th>\n",
       "      <th>material</th>\n",
       "      <th>author</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_processed</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>913336</th>\n",
       "      <td>B087LZNPHS</td>\n",
       "      <td>UK</td>\n",
       "      <td>SOCHOW Sherpa Fleece Throw Blanket, Double-Sid...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>SOCHOW</td>\n",
       "      <td>Teal Green</td>\n",
       "      <td>127cm√ó150cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% Polyester</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLOR: The sherpa throw blanket is available i...</td>\n",
       "      <td>[color, sherpa, throw, blanket, avail, varieti...</td>\n",
       "      <td>[sochow, sherpa, fleec, throw, blanket, double...</td>\n",
       "      <td>[color, sherpa, throw, blanket, avail, varieti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913337</th>\n",
       "      <td>B08THFN1KX</td>\n",
       "      <td>UK</td>\n",
       "      <td>Hippowarehouse Personalised Photo Printed Mous...</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Hippowarehouse</td>\n",
       "      <td>White</td>\n",
       "      <td>240mm x 190mm x 60mm</td>\n",
       "      <td>50245-Mat-Perso</td>\n",
       "      <td>Rubber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Competitively priced</td>\n",
       "      <td>[competit, price]</td>\n",
       "      <td>[hippowarehous, personalis, photo, print, mous...</td>\n",
       "      <td>[competit, price, hippowarehous, personalis, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913338</th>\n",
       "      <td>0804185328</td>\n",
       "      <td>UK</td>\n",
       "      <td>500 Easy Recipes for Every Machine, Both Stove...</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Clarkson Potter</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scarbrough, Mark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[easi, recip, everi, machin, stovetop, electr,...</td>\n",
       "      <td>[easi, recip, everi, machin, stovetop, electr,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913339</th>\n",
       "      <td>B09VBKDBW6</td>\n",
       "      <td>UK</td>\n",
       "      <td>TYHJOY Mini Bag Sealer, Handheld Vacuum Heat S...</td>\n",
       "      <td>11.99</td>\n",
       "      <td>TYHJOY</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FBA-sealer-black</td>\n",
       "      <td>Acrylonitrile Butadiene Styrene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>„ÄêAFTER-SALE„ÄëThis handheld food heat sealer sho...</td>\n",
       "      <td>[aftersalethi, handheld, food, heat, sealer, p...</td>\n",
       "      <td>[tyhjoy, mini, bag, sealer, handheld, vacuum, ...</td>\n",
       "      <td>[aftersalethi, handheld, food, heat, sealer, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913340</th>\n",
       "      <td>B096ZW8B49</td>\n",
       "      <td>UK</td>\n",
       "      <td>Lucosobie Steering Wheel Lock - Car Anti-Theft...</td>\n",
       "      <td>26.99</td>\n",
       "      <td>Lucosobie</td>\n",
       "      <td>Black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alloy Steel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üîê„Äê Anti-Friction &amp; Customer First„ÄëEach box of ...</td>\n",
       "      <td>[antifrict, custom, firsteach, box, product, e...</td>\n",
       "      <td>[lucosobi, steer, wheel, lock, car, antitheft,...</td>\n",
       "      <td>[antifrict, custom, firsteach, box, product, e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id locale                                              title  \\\n",
       "913336  B087LZNPHS     UK  SOCHOW Sherpa Fleece Throw Blanket, Double-Sid...   \n",
       "913337  B08THFN1KX     UK  Hippowarehouse Personalised Photo Printed Mous...   \n",
       "913338  0804185328     UK  500 Easy Recipes for Every Machine, Both Stove...   \n",
       "913339  B09VBKDBW6     UK  TYHJOY Mini Bag Sealer, Handheld Vacuum Heat S...   \n",
       "913340  B096ZW8B49     UK  Lucosobie Steering Wheel Lock - Car Anti-Theft...   \n",
       "\n",
       "        price            brand       color                  size  \\\n",
       "913336  24.99           SOCHOW  Teal Green           127cm√ó150cm   \n",
       "913337   9.95   Hippowarehouse       White  240mm x 190mm x 60mm   \n",
       "913338  16.49  Clarkson Potter       White                   NaN   \n",
       "913339  11.99           TYHJOY       Black                   NaN   \n",
       "913340  26.99        Lucosobie       Black                   NaN   \n",
       "\n",
       "                   model                         material            author  \\\n",
       "913336               NaN                   100% Polyester               NaN   \n",
       "913337   50245-Mat-Perso                           Rubber               NaN   \n",
       "913338               NaN                              NaN  Scarbrough, Mark   \n",
       "913339  FBA-sealer-black  Acrylonitrile Butadiene Styrene               NaN   \n",
       "913340               NaN                      Alloy Steel               NaN   \n",
       "\n",
       "                                                     desc  \\\n",
       "913336  COLOR: The sherpa throw blanket is available i...   \n",
       "913337                               Competitively priced   \n",
       "913338                                                NaN   \n",
       "913339  „ÄêAFTER-SALE„ÄëThis handheld food heat sealer sho...   \n",
       "913340  üîê„Äê Anti-Friction & Customer First„ÄëEach box of ...   \n",
       "\n",
       "                                           desc_processed  \\\n",
       "913336  [color, sherpa, throw, blanket, avail, varieti...   \n",
       "913337                                  [competit, price]   \n",
       "913338                                                 []   \n",
       "913339  [aftersalethi, handheld, food, heat, sealer, p...   \n",
       "913340  [antifrict, custom, firsteach, box, product, e...   \n",
       "\n",
       "                                          title_processed  \\\n",
       "913336  [sochow, sherpa, fleec, throw, blanket, double...   \n",
       "913337  [hippowarehous, personalis, photo, print, mous...   \n",
       "913338  [easi, recip, everi, machin, stovetop, electr,...   \n",
       "913339  [tyhjoy, mini, bag, sealer, handheld, vacuum, ...   \n",
       "913340  [lucosobi, steer, wheel, lock, car, antitheft,...   \n",
       "\n",
       "                                       combined_processed  \n",
       "913336  [color, sherpa, throw, blanket, avail, varieti...  \n",
       "913337  [competit, price, hippowarehous, personalis, p...  \n",
       "913338  [easi, recip, everi, machin, stovetop, electr,...  \n",
       "913339  [aftersalethi, handheld, food, heat, sealer, p...  \n",
       "913340  [antifrict, custom, firsteach, box, product, e...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# # Download required resources from NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocessing functions\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove brackets and non-alphabetic characters\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Sentence tokenization\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "        # Word tokenization, Stemming, Lemmatization, and Stop Word Removal\n",
    "        preprocessed_words = []\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            stemmed_words = [stemmer.stem(word) for word in words]\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "            filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
    "            preprocessed_words.extend(filtered_words)\n",
    "\n",
    "        return preprocessed_words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the text preprocessing pipeline to the 'desc' column\n",
    "uk_samples['desc_processed'] = uk_samples['desc'].apply(lambda x: preprocess(x))\n",
    "uk_samples['title_processed'] = uk_samples['title'].apply(lambda x: preprocess(x))\n",
    "\n",
    "# Create a new column that combines the words from 'desc_processed' and 'title_processed'\n",
    "uk_samples['combined_processed'] = uk_samples['desc_processed'] + uk_samples['title_processed']\n",
    "\n",
    "# Print the updated DataFram\n",
    "uk_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c56f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = uk_samples[uk_samples['id'] == 'B087LZNPHS']['combined_processed']\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01133d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = uk_samples[uk_samples['id'] == 'B087LZNPHS']['unique_words']\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55318b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2090535</th>\n",
       "      <td>['B0BFDL54Y7' 'B0BFDR9X13' 'B07J4WF8VH' 'B07Y2...</td>\n",
       "      <td>B07Y227WNJ</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090536</th>\n",
       "      <td>['B07FM2GLNQ' 'B07GZW3P4W']</td>\n",
       "      <td>B095NNZCR6</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090537</th>\n",
       "      <td>['B0021L95HU' 'B07DDL77RY' 'B07DDL77RY']</td>\n",
       "      <td>B002KA1FZC</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090538</th>\n",
       "      <td>['B0B2WSZYL2' 'B000I8XZ7O']</td>\n",
       "      <td>B000I90TAO</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090539</th>\n",
       "      <td>['B00LW1APOC' 'B00LW1APOC' 'B00OI6NQUI' 'B09HL...</td>\n",
       "      <td>B07H54NZ3K</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prev_items   next_item locale\n",
       "2090535  ['B0BFDL54Y7' 'B0BFDR9X13' 'B07J4WF8VH' 'B07Y2...  B07Y227WNJ     UK\n",
       "2090536                        ['B07FM2GLNQ' 'B07GZW3P4W']  B095NNZCR6     UK\n",
       "2090537           ['B0021L95HU' 'B07DDL77RY' 'B07DDL77RY']  B002KA1FZC     UK\n",
       "2090538                        ['B0B2WSZYL2' 'B000I8XZ7O']  B000I90TAO     UK\n",
       "2090539  ['B00LW1APOC' 'B00LW1APOC' 'B00OI6NQUI' 'B09HL...  B07H54NZ3K     UK"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_session = pd.read_csv(\"sessions_train.csv\")\n",
    "uk_df = df_session[df_session['locale'] == 'UK'].copy()\n",
    "uk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aba66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(ids_str):\n",
    "    ids_str = ids_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    return [id_.strip() for id_ in ids_str.split(\" \") if id_.strip() != ' ']\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to each element of the 'prev_items' column and assign the result to a new column 'prev_item_ids'\n",
    "uk_df['prev_item_ids'] = uk_df['prev_items'].apply(extract_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c25cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "      <th>prev_item_ids</th>\n",
       "      <th>prev_item_unique_words</th>\n",
       "      <th>next_item_unique_words</th>\n",
       "      <th>prev_sentence</th>\n",
       "      <th>next_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2090535</th>\n",
       "      <td>['B0BFDL54Y7' 'B0BFDR9X13' 'B07J4WF8VH' 'B07Y2...</td>\n",
       "      <td>B07Y227WNJ</td>\n",
       "      <td>UK</td>\n",
       "      <td>[B0BFDL54Y7, B0BFDR9X13, B07J4WF8VH, B07Y21LDJX]</td>\n",
       "      <td>[easi, cleanth, blade, easi, remov, easi, clea...</td>\n",
       "      <td>[easi, clean, adov, veget, slicer, complet, di...</td>\n",
       "      <td>easi cleanth blade easi remov easi clean thoro...</td>\n",
       "      <td>easi clean adov veget slicer complet disassemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090536</th>\n",
       "      <td>['B07FM2GLNQ' 'B07GZW3P4W']</td>\n",
       "      <td>B095NNZCR6</td>\n",
       "      <td>UK</td>\n",
       "      <td>[B07FM2GLNQ, B07GZW3P4W]</td>\n",
       "      <td>[accur, control, ergonom, dont, bore, thi, pc,...</td>\n",
       "      <td>[adopt, pure, plain, color, voil, curtain, dro...</td>\n",
       "      <td>accur control ergonom dont bore thi pc mous ac...</td>\n",
       "      <td>adopt pure plain color voil curtain drop pair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090537</th>\n",
       "      <td>['B0021L95HU' 'B07DDL77RY' 'B07DDL77RY']</td>\n",
       "      <td>B002KA1FZC</td>\n",
       "      <td>UK</td>\n",
       "      <td>[B0021L95HU, B07DDL77RY, B07DDL77RY]</td>\n",
       "      <td>[conveni, dosag, seven, sea, cod, liver, oil, ...</td>\n",
       "      <td>[contain, natur, sourc, omega, fish, oil, cod,...</td>\n",
       "      <td>conveni dosag seven sea cod liver oil tablet p...</td>\n",
       "      <td>contain natur sourc omega fish oil cod liver o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090538</th>\n",
       "      <td>['B0B2WSZYL2' 'B000I8XZ7O']</td>\n",
       "      <td>B000I90TAO</td>\n",
       "      <td>UK</td>\n",
       "      <td>[B0B2WSZYL2, B000I8XZ7O]</td>\n",
       "      <td>[suit, az, ax, hv, hv, hv, nv, nv, hv, nv, nv,...</td>\n",
       "      <td>[includ, hood, jumpsuit, rubi, offici, hallowe...</td>\n",
       "      <td>suit az ax hv hv hv nv nv hv nv nv nv nv nv nv...</td>\n",
       "      <td>includ hood jumpsuit rubi offici halloween hau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090539</th>\n",
       "      <td>['B00LW1APOC' 'B00LW1APOC' 'B00OI6NQUI' 'B09HL...</td>\n",
       "      <td>B07H54NZ3K</td>\n",
       "      <td>UK</td>\n",
       "      <td>[B00LW1APOC, B00LW1APOC, B00OI6NQUI, B09HLDN8W1]</td>\n",
       "      <td>[damagefre, decor, say, goodby, hole, sticki, ...</td>\n",
       "      <td>[damag, free, hang, damag, free, hang, strip, ...</td>\n",
       "      <td>damagefre decor say goodby hole sticki residu ...</td>\n",
       "      <td>damag free hang damag free hang strip work wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prev_items   next_item locale  \\\n",
       "2090535  ['B0BFDL54Y7' 'B0BFDR9X13' 'B07J4WF8VH' 'B07Y2...  B07Y227WNJ     UK   \n",
       "2090536                        ['B07FM2GLNQ' 'B07GZW3P4W']  B095NNZCR6     UK   \n",
       "2090537           ['B0021L95HU' 'B07DDL77RY' 'B07DDL77RY']  B002KA1FZC     UK   \n",
       "2090538                        ['B0B2WSZYL2' 'B000I8XZ7O']  B000I90TAO     UK   \n",
       "2090539  ['B00LW1APOC' 'B00LW1APOC' 'B00OI6NQUI' 'B09HL...  B07H54NZ3K     UK   \n",
       "\n",
       "                                            prev_item_ids  \\\n",
       "2090535  [B0BFDL54Y7, B0BFDR9X13, B07J4WF8VH, B07Y21LDJX]   \n",
       "2090536                          [B07FM2GLNQ, B07GZW3P4W]   \n",
       "2090537              [B0021L95HU, B07DDL77RY, B07DDL77RY]   \n",
       "2090538                          [B0B2WSZYL2, B000I8XZ7O]   \n",
       "2090539  [B00LW1APOC, B00LW1APOC, B00OI6NQUI, B09HLDN8W1]   \n",
       "\n",
       "                                    prev_item_unique_words  \\\n",
       "2090535  [easi, cleanth, blade, easi, remov, easi, clea...   \n",
       "2090536  [accur, control, ergonom, dont, bore, thi, pc,...   \n",
       "2090537  [conveni, dosag, seven, sea, cod, liver, oil, ...   \n",
       "2090538  [suit, az, ax, hv, hv, hv, nv, nv, hv, nv, nv,...   \n",
       "2090539  [damagefre, decor, say, goodby, hole, sticki, ...   \n",
       "\n",
       "                                    next_item_unique_words  \\\n",
       "2090535  [easi, clean, adov, veget, slicer, complet, di...   \n",
       "2090536  [adopt, pure, plain, color, voil, curtain, dro...   \n",
       "2090537  [contain, natur, sourc, omega, fish, oil, cod,...   \n",
       "2090538  [includ, hood, jumpsuit, rubi, offici, hallowe...   \n",
       "2090539  [damag, free, hang, damag, free, hang, strip, ...   \n",
       "\n",
       "                                             prev_sentence  \\\n",
       "2090535  easi cleanth blade easi remov easi clean thoro...   \n",
       "2090536  accur control ergonom dont bore thi pc mous ac...   \n",
       "2090537  conveni dosag seven sea cod liver oil tablet p...   \n",
       "2090538  suit az ax hv hv hv nv nv hv nv nv nv nv nv nv...   \n",
       "2090539  damagefre decor say goodby hole sticki residu ...   \n",
       "\n",
       "                                             next_sentence  \n",
       "2090535  easi clean adov veget slicer complet disassemb...  \n",
       "2090536  adopt pure plain color voil curtain drop pair ...  \n",
       "2090537  contain natur sourc omega fish oil cod liver o...  \n",
       "2090538  includ hood jumpsuit rubi offici halloween hau...  \n",
       "2090539  damag free hang damag free hang strip work wit...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Get the unique words from uk_samples\n",
    "unique_words_dict = dict(zip(uk_samples['id'], uk_samples['combined_processed']))\n",
    "\n",
    "# Step 2: Define a function to retrieve unique words for a single item ID\n",
    "def get_unique_words(item_id):\n",
    "    unique_words = []\n",
    "    if item_id in unique_words_dict:\n",
    "        unique_words = unique_words_dict[item_id]\n",
    "    return unique_words\n",
    "\n",
    "# Step 3: Apply the function to the 'prev_item_ids' column in uk_df\n",
    "uk_df['prev_item_unique_words'] = uk_df['prev_item_ids'].apply(lambda x: [word for item_id in x for word in get_unique_words(item_id)])\n",
    "# Step 4: Apply the function to the 'next_item' column in uk_df\n",
    "uk_df['next_item_unique_words'] = uk_df['next_item'].apply(lambda x: get_unique_words(x) if x in unique_words_dict else [])\n",
    "\n",
    "\n",
    "# Step 5: Join the words into sentences\n",
    "uk_df['prev_sentence'] = uk_df['prev_item_unique_words'].apply(lambda x: ' '.join(x))\n",
    "uk_df['next_sentence'] = uk_df['next_item_unique_words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "uk_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030f1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import numpy as np\n",
    "\n",
    "# # Train Word2Vec model on the preprocessed 'desc' data\n",
    "# model = Word2Vec(sentences=uk_df['prev_sentence'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Function to calculate the word embedding vector for a specific product\n",
    "# def calculate_product_embedding(desc):\n",
    "\n",
    "#     if isinstance(desc, str):\n",
    "        \n",
    "#         desc_embedding = np.zeros(model.vector_size)\n",
    "#         num_desc_words = 0\n",
    "#         for word in desc.split():\n",
    "#             if word in model.wv:\n",
    "#                 desc_embedding += model.wv[word]\n",
    "#                 num_desc_words += 1\n",
    "\n",
    "#         if num_desc_words > 0:\n",
    "#             desc_embedding /= num_desc_words\n",
    "#             return desc_embedding\n",
    "    \n",
    "#     return None\n",
    "\n",
    "\n",
    "\n",
    "# uk_df['X_embeddings'] = uk_df.apply(lambda row: calculate_product_embedding(row['prev_sentence']), axis=1)\n",
    "# uk_df['Y_embeddings'] = uk_df.apply(lambda row: calculate_product_embedding(row['next_sentence']), axis=1)\n",
    "\n",
    "# uk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f3bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "sample = uk_df.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da2a01df",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2180/145566764.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Fit the vectorizer on the preprocessed 'desc' data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prev_sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Function to calculate the TF-IDF embedding vector for a specific product\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1123\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the preprocessed 'desc' data\n",
    "vectorizer.fit(uk_df['prev_sentence'])\n",
    "\n",
    "# Function to calculate the TF-IDF embedding vector for a specific product\n",
    "def calculate_product_embedding(desc):\n",
    "    if isinstance(desc, str):\n",
    "        desc_embedding = vectorizer.transform([desc]).toarray()[0]\n",
    "        return desc_embedding\n",
    "    return None\n",
    "\n",
    "sample['X_embeddings'] = sample.apply(lambda row: calculate_product_embedding(row['prev_sentence']), axis=1)\n",
    "sample['Y_embeddings'] = sample.apply(lambda row: calculate_product_embedding(row['next_sentence']), axis=1)\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uk_samples.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a73f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uk_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_df['X_embeddings'].dropna()\n",
    "uk_df['Y_embeddings'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ac484d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X_embeddings'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2180/2497598470.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Extract the input and target values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_embeddings'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y_embeddings'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3455\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'X_embeddings'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Extract the input and target values\n",
    "X = np.stack(sample['X_embeddings'].values)\n",
    "y = np.stack(sample['Y_embeddings'].values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Step 3: Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "# Output layer with the same dimension as y_train\n",
    "learning_rate = 0.01  # Set the desired learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Extract the input and target values\n",
    "X = np.stack(uk_df['X_embeddings'].values)\n",
    "y = np.stack(uk_df['Y_embeddings'].values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Step 3: Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcf037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Extract the input and target values\n",
    "X = np.stack(sample['X_embeddings'].values)\n",
    "y = np.stack(sample['Y_embeddings'].values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Step 3: Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa26554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "\n",
    "# Extract the input and target values\n",
    "X = np.stack(sample['X_embeddings'].values)\n",
    "y = np.stack(sample['Y_embeddings'].values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Step 3: Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(100, activation='softmax')) \n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf34210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
